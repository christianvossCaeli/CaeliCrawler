# CaeliCrawler - Implementierungsplan

## Executive Summary

CaeliCrawler ist eine interne Datensammlungsplattform zur automatisierten Erfassung und Analyse von kommunalen Informationen (z.B. GemeinderatsbeschlÃ¼sse zu Windkraft-Restriktionen). Die Plattform kombiniert Web-Crawling, API-Integration, PDF-Verarbeitung und KI-gestÃ¼tzte Datenanalyse.

---

## 1. Technologie-Stack

### Backend-Framework: **Python mit FastAPI**

**BegrÃ¼ndung:**
- Hervorragende KI/ML-Integration (Azure OpenAI, LangChain)
- Asynchrone Verarbeitung nativ unterstÃ¼tzt
- Hohe Performance bei I/O-bound Tasks
- Einfache Deployment auf Managed Servern
- Umfangreiches Ã–kosystem fÃ¼r Web-Scraping (Scrapy, Playwright)

### Komponenten-Ãœbersicht

| Komponente | Technologie | Zweck |
|------------|-------------|-------|
| API/Backend | FastAPI | REST-API, Admin-Backend |
| Frontend | Vue.js 3 + Vuetify | Admin-Dashboard |
| Task Queue | Celery + Redis | Asynchrone Crawler-Jobs |
| Datenbank | PostgreSQL | Strukturierte Daten |
| Dokumenten-Index | Elasticsearch (optional) | Volltextsuche Ã¼ber PDFs |
| PDF-Verarbeitung | PyMuPDF + Azure AI Document Intelligence | Text-Extraktion |
| Web-Crawling | Scrapy + Playwright | Seitennavigation & Scraping |
| KI-Analyse | Azure OpenAI (GPT-4o) | Inhaltsanalyse & Klassifizierung |
| Change Detection | Custom Scheduler | Ã„nderungserkennung |

---

## 2. Systemarchitektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              FRONTEND (Vue.js 3)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Dashboard   â”‚  â”‚  Kategorien  â”‚  â”‚  Datenquellenâ”‚  â”‚  Ergebnisse  â”‚    â”‚
â”‚  â”‚  & Status    â”‚  â”‚  verwalten   â”‚  â”‚  konfigurierenâ”‚  â”‚  & Export   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FASTAPI BACKEND                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  /api/admin  â”‚  â”‚ /api/sources â”‚  â”‚ /api/crawlersâ”‚  â”‚  /api/data   â”‚    â”‚
â”‚  â”‚  Kategorien  â”‚  â”‚  URL-Mgmt    â”‚  â”‚  Job-Control â”‚  â”‚  Output API  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CELERY WORKER    â”‚  â”‚   REDIS BROKER   â”‚  â”‚    POSTGRESQL DB     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚                  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Crawler Tasks  â”‚  â”‚  â”‚  - Task Queue    â”‚  â”‚  â”‚ Kategorien     â”‚  â”‚
â”‚  â”‚ PDF Processing â”‚  â”‚  â”‚  - Result Store  â”‚  â”‚  â”‚ Datenquellen   â”‚  â”‚
â”‚  â”‚ AI Analysis    â”‚  â”‚  â”‚  - Cache         â”‚  â”‚  â”‚ Crawl-Results  â”‚  â”‚
â”‚  â”‚ Change Detectionâ”‚ â”‚  â”‚  - Rate Limiting â”‚  â”‚  â”‚ Dokumente      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚                  â”‚  â”‚  â”‚ Ã„nderungslog   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EXTERNE DIENSTE                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Azure OpenAI â”‚  â”‚  OParl APIs  â”‚  â”‚  Gemeinde-   â”‚  â”‚  Webhook     â”‚    â”‚
â”‚  â”‚ GPT-4o       â”‚  â”‚  (Kommunen)  â”‚  â”‚  Websites    â”‚  â”‚  Endpunkte   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Datenmodell

### 3.1 HauptentitÃ¤ten

```python
# categories - Kategorien (z.B. "Gemeinden", "Landkreise")
class Category:
    id: UUID
    name: str                          # "Gemeinden"
    description: str                   # Beschreibung
    purpose: str                       # "Windkraft-Restriktionen analysieren"
    search_terms: list[str]            # ["Windkraft", "Bebauungsplan", "FlÃ¤chennutzung"]
    document_types: list[str]          # ["Gemeinderatsbeschluss", "Sitzungsprotokoll"]
    ai_extraction_prompt: str          # Custom Prompt fÃ¼r KI-Analyse
    schedule_cron: str                 # "0 2 * * *" (tÃ¤glich 2 Uhr)
    is_active: bool
    created_at: datetime
    updated_at: datetime

# data_sources - Datenquellen pro Kategorie
class DataSource:
    id: UUID
    category_id: UUID                  # FK zu Category
    name: str                          # "Gemeinde Musterstadt"
    source_type: SourceType            # WEBSITE | OPARL_API | RSS | CUSTOM_API
    base_url: str                      # "https://gemeinde-musterstadt.de"
    api_endpoint: str | None           # OParl Endpoint falls vorhanden
    crawl_config: dict                 # Scraping-Konfiguration (Selektoren etc.)
    auth_config: dict | None           # API-Keys, Login-Daten (verschlÃ¼sselt)
    last_crawl: datetime | None
    last_change_detected: datetime | None
    content_hash: str | None           # FÃ¼r Change Detection
    status: SourceStatus               # ACTIVE | PAUSED | ERROR | PENDING
    error_message: str | None
    metadata: dict                     # ZusÃ¤tzliche Infos (Bundesland, Einwohner, etc.)

# crawl_jobs - Crawling-AuftrÃ¤ge
class CrawlJob:
    id: UUID
    source_id: UUID
    category_id: UUID
    status: JobStatus                  # PENDING | RUNNING | COMPLETED | FAILED
    started_at: datetime | None
    completed_at: datetime | None
    pages_crawled: int
    documents_found: int
    documents_processed: int
    error_log: list[str]
    stats: dict                        # Detaillierte Statistiken

# documents - Gefundene Dokumente
class Document:
    id: UUID
    source_id: UUID
    category_id: UUID
    crawl_job_id: UUID
    document_type: str                 # "PDF" | "HTML" | "DOC"
    original_url: str
    title: str
    file_path: str | None              # Lokaler Speicherpfad
    file_hash: str                     # SHA256 fÃ¼r Duplikat-Erkennung
    raw_text: str | None               # Extrahierter Rohtext
    page_count: int | None
    file_size: int
    discovered_at: datetime
    processed_at: datetime | None
    processing_status: ProcessingStatus

# extracted_data - KI-extrahierte Informationen
class ExtractedData:
    id: UUID
    document_id: UUID
    category_id: UUID
    extraction_type: str               # "Windkraft-Beschluss", "FlÃ¤chennutzung"
    extracted_content: dict            # Strukturierte extrahierte Daten
    confidence_score: float            # 0.0 - 1.0
    ai_model_used: str                 # "gpt-4o-2024-08-06"
    ai_prompt_version: str
    raw_ai_response: str
    human_verified: bool
    human_corrections: dict | None
    created_at: datetime
    updated_at: datetime

# change_log - Ã„nderungsprotokoll
class ChangeLog:
    id: UUID
    source_id: UUID
    detected_at: datetime
    change_type: ChangeType            # NEW_DOCUMENT | CONTENT_CHANGED | REMOVED
    old_hash: str | None
    new_hash: str
    affected_url: str
    details: dict

# api_exports - Konfigurierte Export-Endpunkte
class ApiExport:
    id: UUID
    name: str
    category_id: UUID | None           # NULL = alle Kategorien
    endpoint_type: ExportType          # INTERNAL_API | WEBHOOK | PUSH_TO_EXTERNAL
    config: dict                       # URL, Auth, Format, Filter
    last_export: datetime | None
    is_active: bool
```

### 3.2 PostgreSQL Schema

```sql
-- Enums
CREATE TYPE source_type AS ENUM ('WEBSITE', 'OPARL_API', 'RSS', 'CUSTOM_API');
CREATE TYPE source_status AS ENUM ('ACTIVE', 'PAUSED', 'ERROR', 'PENDING');
CREATE TYPE job_status AS ENUM ('PENDING', 'RUNNING', 'COMPLETED', 'FAILED');
CREATE TYPE processing_status AS ENUM ('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED');
CREATE TYPE change_type AS ENUM ('NEW_DOCUMENT', 'CONTENT_CHANGED', 'REMOVED');
CREATE TYPE export_type AS ENUM ('INTERNAL_API', 'WEBHOOK', 'PUSH_TO_EXTERNAL');

-- Kategorien
CREATE TABLE categories (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL UNIQUE,
    description TEXT,
    purpose TEXT NOT NULL,
    search_terms JSONB DEFAULT '[]',
    document_types JSONB DEFAULT '[]',
    ai_extraction_prompt TEXT,
    schedule_cron VARCHAR(100) DEFAULT '0 2 * * *',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Datenquellen
CREATE TABLE data_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    category_id UUID NOT NULL REFERENCES categories(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    source_type source_type NOT NULL,
    base_url TEXT NOT NULL,
    api_endpoint TEXT,
    crawl_config JSONB DEFAULT '{}',
    auth_config JSONB,
    last_crawl TIMESTAMPTZ,
    last_change_detected TIMESTAMPTZ,
    content_hash VARCHAR(64),
    status source_status DEFAULT 'PENDING',
    error_message TEXT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(category_id, base_url)
);

-- Index fÃ¼r schnelle Suche nach Status
CREATE INDEX idx_sources_status ON data_sources(status);
CREATE INDEX idx_sources_category ON data_sources(category_id);
CREATE INDEX idx_sources_last_crawl ON data_sources(last_crawl);

-- Crawl Jobs
CREATE TABLE crawl_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID NOT NULL REFERENCES data_sources(id) ON DELETE CASCADE,
    category_id UUID NOT NULL REFERENCES categories(id),
    status job_status DEFAULT 'PENDING',
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    pages_crawled INTEGER DEFAULT 0,
    documents_found INTEGER DEFAULT 0,
    documents_processed INTEGER DEFAULT 0,
    error_log JSONB DEFAULT '[]',
    stats JSONB DEFAULT '{}'
);

CREATE INDEX idx_jobs_status ON crawl_jobs(status);
CREATE INDEX idx_jobs_source ON crawl_jobs(source_id);

-- Dokumente
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID NOT NULL REFERENCES data_sources(id) ON DELETE CASCADE,
    category_id UUID NOT NULL REFERENCES categories(id),
    crawl_job_id UUID REFERENCES crawl_jobs(id),
    document_type VARCHAR(50) NOT NULL,
    original_url TEXT NOT NULL,
    title TEXT,
    file_path TEXT,
    file_hash VARCHAR(64) NOT NULL,
    raw_text TEXT,
    page_count INTEGER,
    file_size BIGINT,
    discovered_at TIMESTAMPTZ DEFAULT NOW(),
    processed_at TIMESTAMPTZ,
    processing_status processing_status DEFAULT 'PENDING',
    UNIQUE(source_id, file_hash)
);

CREATE INDEX idx_documents_status ON documents(processing_status);
CREATE INDEX idx_documents_source ON documents(source_id);
CREATE INDEX idx_documents_hash ON documents(file_hash);

-- Extrahierte Daten
CREATE TABLE extracted_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    category_id UUID NOT NULL REFERENCES categories(id),
    extraction_type VARCHAR(255) NOT NULL,
    extracted_content JSONB NOT NULL,
    confidence_score FLOAT CHECK (confidence_score >= 0 AND confidence_score <= 1),
    ai_model_used VARCHAR(100),
    ai_prompt_version VARCHAR(50),
    raw_ai_response TEXT,
    human_verified BOOLEAN DEFAULT false,
    human_corrections JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_extracted_document ON extracted_data(document_id);
CREATE INDEX idx_extracted_type ON extracted_data(extraction_type);
CREATE INDEX idx_extracted_verified ON extracted_data(human_verified);

-- Ã„nderungsprotokoll
CREATE TABLE change_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID NOT NULL REFERENCES data_sources(id) ON DELETE CASCADE,
    detected_at TIMESTAMPTZ DEFAULT NOW(),
    change_type change_type NOT NULL,
    old_hash VARCHAR(64),
    new_hash VARCHAR(64),
    affected_url TEXT,
    details JSONB DEFAULT '{}'
);

CREATE INDEX idx_changelog_source ON change_log(source_id);
CREATE INDEX idx_changelog_date ON change_log(detected_at DESC);

-- API Export Konfigurationen
CREATE TABLE api_exports (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    category_id UUID REFERENCES categories(id),
    endpoint_type export_type NOT NULL,
    config JSONB NOT NULL,
    last_export TIMESTAMPTZ,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Volltext-Suche Ã¼ber Dokumente (PostgreSQL native)
ALTER TABLE documents ADD COLUMN search_vector tsvector;
CREATE INDEX idx_documents_search ON documents USING GIN(search_vector);

-- Trigger fÃ¼r automatische Aktualisierung des Suchvektors
CREATE OR REPLACE FUNCTION update_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector := to_tsvector('german', COALESCE(NEW.title, '') || ' ' || COALESCE(NEW.raw_text, ''));
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER documents_search_update
    BEFORE INSERT OR UPDATE ON documents
    FOR EACH ROW EXECUTE FUNCTION update_search_vector();
```

---

## 4. VerfÃ¼gbare Datenquellen & APIs

### 4.1 OParl - Offene Parlamentarische Informationen

**Was ist OParl?**
OParl ist ein deutscher Standard fÃ¼r den offenen Zugriff auf kommunale Ratsinformationssysteme.

**Technische Details:**
- REST-API mit JSON-Responses
- Keine Authentifizierung erforderlich (Ã¶ffentliche Daten)
- Version: OParl 1.1

**Beispiel-Endpunkte:**
```
# System-Informationen
https://oparl.stadt-muenster.de/system

# KÃ¶rperschaften (Gemeinden, Landkreise)
https://oparl.stadt-muenster.de/bodies

# Sitzungen mit Tagesordnungen
https://oparl.stadt-muenster.de/meetings

# Drucksachen und BeschlÃ¼sse
https://oparl.stadt-muenster.de/papers
```

**VerfÃ¼gbare Kommunen (Auswahl):**
- KÃ¶ln, DÃ¼sseldorf, MÃ¼nster, Aachen
- 27+ Kommunen in NRW Ã¼ber Open.NRW
- Weitere Ã¼ber [Politik bei uns](https://politik-bei-uns.de/)

**Quellen:**
- [OParl Hauptseite](https://oparl.org/)
- [OParl fÃ¼r Entwickler](https://oparl.org/oparl-fuer-entwickler/)
- [NRW OParl-Kommunen](https://open.nrw/open-data/showroom/nutzung-von-oparl-kommunen-aus-nrw)

### 4.2 Politik bei uns

**Beschreibung:** Aggregator fÃ¼r OParl-Daten mit erweiterter Suche.

**Features:**
- Volltextsuche Ã¼ber alle Kommunen
- Geolokalisierung von BeschlÃ¼ssen
- GitHub: [politik-bei-uns](https://github.com/politik-bei-uns)

### 4.3 GovData

**URL:** https://www.govdata.de/

**Beschreibung:** Zentrales Datenportal fÃ¼r Open Government Data in Deutschland.

**Nutzung:** Metadaten-Suche fÃ¼r verfÃ¼gbare DatensÃ¤tze auf allen Verwaltungsebenen.

### 4.4 GENESIS-Datenbank

**URL:** https://www-genesis.destatis.de/

**Beschreibung:** Statistische Daten zu allen Gemeinden (Einwohner, FlÃ¤che, etc.).

**API:** RESTful JSON-Schnittstelle verfÃ¼gbar.

### 4.5 XPlanung24

**URL:** https://xplanung24.de/

**Beschreibung:** Digitale Bauleitplanung mit 400+ StÃ¤dten und Gemeinden.

**Relevanz:** FlÃ¤chennutzungsplÃ¤ne, BebauungsplÃ¤ne (relevant fÃ¼r Windkraft-Standorte).

---

## 5. Crawler-Strategie

### 5.1 Multi-Source Crawler Architektur

```python
# Abstrakte Basis fÃ¼r verschiedene Crawler-Typen
class BaseCrawler(ABC):
    @abstractmethod
    async def crawl(self, source: DataSource) -> CrawlResult:
        pass

    @abstractmethod
    async def detect_changes(self, source: DataSource) -> list[Change]:
        pass

class OparlCrawler(BaseCrawler):
    """Spezialisierter Crawler fÃ¼r OParl-APIs"""

class WebsiteCrawler(BaseCrawler):
    """Scrapy-basierter Crawler fÃ¼r regulÃ¤re Websites"""

class RSSCrawler(BaseCrawler):
    """Crawler fÃ¼r RSS/Atom Feeds"""
```

### 5.2 Crawling-Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCHEDULER (Celery Beat)                       â”‚
â”‚                                                                   â”‚
â”‚  PrÃ¼ft alle aktiven Kategorien gemÃ¤ÃŸ schedule_cron               â”‚
â”‚  Erstellt CrawlJobs fÃ¼r fÃ¤llige DataSources                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHANGE DETECTION                               â”‚
â”‚                                                                   â”‚
â”‚  1. HEAD Request oder Hash-Vergleich                             â”‚
â”‚  2. OParl: PrÃ¼fe "modified" Timestamps                           â”‚
â”‚  3. Websites: Compare content_hash                               â”‚
â”‚  4. Bei Ã„nderung â†’ starte Crawl                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CRAWL EXECUTION                              â”‚
â”‚                                                                   â”‚
â”‚  OParl:                                                          â”‚
â”‚  â”œâ”€ GET /papers?modified_after={last_crawl}                      â”‚
â”‚  â””â”€ Iteriere durch Pagination                                    â”‚
â”‚                                                                   â”‚
â”‚  Website:                                                         â”‚
â”‚  â”œâ”€ Scrapy Spider mit konfigurierten Selektoren                  â”‚
â”‚  â”œâ”€ Folge internen Links (max_depth konfigurierbar)             â”‚
â”‚  â””â”€ Playwright fÃ¼r JavaScript-gerenderte Seiten                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DOCUMENT PROCESSING                             â”‚
â”‚                                                                   â”‚
â”‚  1. Download PDFs/Dokumente                                      â”‚
â”‚  2. Deduplizierung via file_hash                                 â”‚
â”‚  3. Text-Extraktion:                                             â”‚
â”‚     - PDF: PyMuPDF / Azure Document Intelligence                 â”‚
â”‚     - HTML: BeautifulSoup                                        â”‚
â”‚     - DOC/DOCX: python-docx                                      â”‚
â”‚  4. Speichere raw_text in documents Tabelle                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AI ANALYSIS                                   â”‚
â”‚                                                                   â”‚
â”‚  1. Lade category.ai_extraction_prompt                           â”‚
â”‚  2. Sende an Azure OpenAI GPT-4o:                                â”‚
â”‚     - System Prompt mit Extraktionsregeln                        â”‚
â”‚     - Document Text als User Content                             â”‚
â”‚     - Structured Output (JSON Schema)                            â”‚
â”‚  3. Speichere in extracted_data mit confidence_score             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NOTIFICATION & EXPORT                           â”‚
â”‚                                                                   â”‚
â”‚  1. Neue relevante Daten erkannt?                                â”‚
â”‚  2. Trigger konfigurierte Webhooks                               â”‚
â”‚  3. Push zu externen APIs falls konfiguriert                     â”‚
â”‚  4. Update Dashboard-Statistiken                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Rate Limiting & Politeness

```python
CRAWL_CONFIG = {
    "default_delay": 2.0,           # Sekunden zwischen Requests
    "max_concurrent_requests": 5,   # Pro Domain
    "respect_robots_txt": True,
    "user_agent": "CaeliCrawler/1.0 (Research; contact@example.com)",
    "retry_times": 3,
    "retry_backoff": "exponential",
}
```

---

## 6. KI-Integration (Azure OpenAI)

### 6.1 Konfiguration

```python
# Azure OpenAI Setup
AZURE_OPENAI_CONFIG = {
    "endpoint": "https://your-resource.openai.azure.com/",
    "api_version": "2024-08-01-preview",
    "deployment_name": "gpt-4o",
    "max_tokens": 4096,
}
```

### 6.2 Beispiel: Windkraft-Extraktions-Prompt

```python
WINDKRAFT_EXTRACTION_PROMPT = """
Du bist ein Experte fÃ¼r die Analyse kommunaler Dokumente bezÃ¼glich Windkraft-Regelungen.

Analysiere das folgende Dokument und extrahiere strukturiert:

1. **Dokumenttyp**: (Beschluss, Protokoll, Satzung, Bebauungsplan, etc.)
2. **Datum**: Wann wurde das Dokument erstellt/beschlossen?
3. **Gemeinde/Kommune**: Welche Kommune betrifft das Dokument?
4. **Windkraft-Relevanz**: (hoch/mittel/gering/keine)
5. **Zusammenfassung**: Kurze Zusammenfassung des Inhalts (max 200 WÃ¶rter)
6. **Restriktionen**: Liste aller erwÃ¤hnten EinschrÃ¤nkungen fÃ¼r Windkraft:
   - Abstandsregelungen (z.B. "1000m zu Wohnbebauung")
   - HÃ¶henbeschrÃ¤nkungen
   - Ausschlussgebiete
   - Sonstige Auflagen
7. **FÃ¶rdernde MaÃŸnahmen**: Liste aller positiven Regelungen fÃ¼r Windkraft
8. **ErwÃ¤hnte Gesetze/Verordnungen**: Referenzen auf andere Rechtsgrundlagen
9. **Status**: (geplant, beschlossen, in Kraft, aufgehoben)

Antworte im JSON-Format.
"""
```

### 6.3 Structured Output Schema

```python
from pydantic import BaseModel, Field

class WindkraftRestriktion(BaseModel):
    typ: str = Field(description="Art der Restriktion")
    wert: str = Field(description="Konkreter Wert/Beschreibung")
    quelle_im_dokument: str = Field(description="Zitat aus dem Dokument")

class WindkraftAnalyse(BaseModel):
    dokumenttyp: str
    datum: str | None
    gemeinde: str
    windkraft_relevanz: Literal["hoch", "mittel", "gering", "keine"]
    zusammenfassung: str
    restriktionen: list[WindkraftRestriktion]
    foerdernde_massnahmen: list[str]
    erwaehnete_gesetze: list[str]
    status: str
    confidence: float = Field(ge=0, le=1)
```

---

## 7. API-Endpunkte

### 7.1 Admin-API

```yaml
# Kategorien
POST   /api/admin/categories              # Neue Kategorie erstellen
GET    /api/admin/categories              # Alle Kategorien auflisten
GET    /api/admin/categories/{id}         # Kategorie-Details
PUT    /api/admin/categories/{id}         # Kategorie aktualisieren
DELETE /api/admin/categories/{id}         # Kategorie lÃ¶schen

# Datenquellen
POST   /api/admin/sources                 # Neue Datenquelle hinzufÃ¼gen
GET    /api/admin/sources                 # Alle Quellen (mit Filter)
GET    /api/admin/sources/{id}            # Quellen-Details
PUT    /api/admin/sources/{id}            # Quelle aktualisieren
DELETE /api/admin/sources/{id}            # Quelle lÃ¶schen
POST   /api/admin/sources/bulk-import     # CSV/JSON Import von URLs

# Crawl-Steuerung
POST   /api/admin/crawl/start             # Crawl manuell starten
POST   /api/admin/crawl/stop/{job_id}     # Crawl stoppen
GET    /api/admin/crawl/status            # Aktuelle Jobs
GET    /api/admin/crawl/history           # Job-Historie

# Dokumente & Daten
GET    /api/admin/documents               # Dokumente durchsuchen
GET    /api/admin/documents/{id}          # Dokument-Details
PUT    /api/admin/extracted/{id}          # Extrahierte Daten korrigieren
POST   /api/admin/extracted/{id}/verify   # Als verifiziert markieren
```

### 7.2 Output-API (fÃ¼r externe Konsumenten)

```yaml
# Ã–ffentliche Daten-API
GET    /api/v1/data                       # Alle extrahierten Daten
GET    /api/v1/data/categories/{slug}     # Daten einer Kategorie
GET    /api/v1/data/sources/{id}          # Daten einer Quelle
GET    /api/v1/search                     # Volltextsuche
GET    /api/v1/changes                    # Ã„nderungsfeed (Polling)

# Export
GET    /api/v1/export/csv                 # CSV-Export
GET    /api/v1/export/json                # JSON-Export
```

### 7.3 Webhook-Integration

```python
# Webhook-Konfiguration
{
    "name": "Windkraft-Updates",
    "url": "https://external-system.com/webhook",
    "events": ["new_document", "data_extracted", "change_detected"],
    "filter": {
        "category": "gemeinden",
        "min_confidence": 0.8
    },
    "auth": {
        "type": "bearer",
        "token": "xxx"
    }
}
```

---

## 8. Frontend (Admin-Dashboard)

### 8.1 Technologie

- **Framework:** Vue.js 3 mit Composition API
- **UI-Bibliothek:** Vuetify 3 (Material Design)
- **State Management:** Pinia
- **Charts:** Chart.js oder Apache ECharts

### 8.2 Hauptbereiche

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SIDEBAR           â”‚  MAIN CONTENT                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  ğŸ“Š Dashboard      â”‚                                            â”‚
â”‚  ğŸ“ Kategorien     â”‚  [Je nach Selektion]                      â”‚
â”‚  ğŸŒ Datenquellen   â”‚                                            â”‚
â”‚  ğŸ”„ Crawler-Status â”‚                                            â”‚
â”‚  ğŸ“„ Dokumente      â”‚                                            â”‚
â”‚  ğŸ“ˆ Ergebnisse     â”‚                                            â”‚
â”‚  âš™ï¸ Einstellungen  â”‚                                            â”‚
â”‚  ğŸ“¤ Export/API     â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 Dashboard-Widgets

1. **Ãœbersichts-Karten:**
   - Gesamtzahl Kategorien / Quellen / Dokumente
   - Aktive Crawler
   - Neue Ã„nderungen (24h)

2. **Crawler-Status-Chart:**
   - Echtzeit-Fortschritt aktiver Jobs
   - Erfolgs-/Fehlerrate

3. **Ã„nderungs-Timeline:**
   - Chronologische Liste neuer Funde
   - Filterbar nach Kategorie

4. **Geografische Karte:**
   - Visualisierung der Gemeinden
   - Farbcodierung nach Status/Ergebnissen

---

## 9. Projektstruktur

```
CaeliCrawler/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI Application
â”‚   â”‚   â”œâ”€â”€ config.py               # Settings & Environment
â”‚   â”‚   â”œâ”€â”€ database.py             # SQLAlchemy Setup
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                 # SQLAlchemy Models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ category.py
â”‚   â”‚   â”‚   â”œâ”€â”€ data_source.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”‚   â”œâ”€â”€ crawl_job.py
â”‚   â”‚   â”‚   â””â”€â”€ extracted_data.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ schemas/                # Pydantic Schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ category.py
â”‚   â”‚   â”‚   â”œâ”€â”€ data_source.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/                    # API Routes
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ admin/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ categories.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sources.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ crawler.py
â”‚   â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚   â”‚       â”œâ”€â”€ data.py
â”‚   â”‚   â”‚       â””â”€â”€ export.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/               # Business Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_service.py
â”‚   â”‚   â”‚   â””â”€â”€ export_service.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ core/                   # Core Utilities
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ security.py
â”‚   â”‚       â””â”€â”€ exceptions.py
â”‚   â”‚
â”‚   â”œâ”€â”€ crawlers/                   # Crawler Implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                 # Abstract Base Crawler
â”‚   â”‚   â”œâ”€â”€ oparl_crawler.py        # OParl API Crawler
â”‚   â”‚   â”œâ”€â”€ website_crawler.py      # Scrapy-based Crawler
â”‚   â”‚   â””â”€â”€ rss_crawler.py          # RSS Feed Crawler
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/                    # Celery Tasks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ celery_app.py           # Celery Configuration
â”‚   â”‚   â”œâ”€â”€ crawl_tasks.py          # Crawling Tasks
â”‚   â”‚   â”œâ”€â”€ processing_tasks.py     # Document Processing
â”‚   â”‚   â””â”€â”€ ai_tasks.py             # AI Analysis Tasks
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/                 # Document Processors
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â”‚   â”œâ”€â”€ html_processor.py
â”‚   â”‚   â””â”€â”€ office_processor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_api/
â”‚   â”‚   â”œâ”€â”€ test_crawlers/
â”‚   â”‚   â””â”€â”€ test_processors/
â”‚   â”‚
â”‚   â”œâ”€â”€ alembic/                    # Database Migrations
â”‚   â”‚   â”œâ”€â”€ versions/
â”‚   â”‚   â””â”€â”€ env.py
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ requirements-dev.txt
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.ts
â”‚   â”‚   â”œâ”€â”€ App.vue
â”‚   â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ stores/                 # Pinia Stores
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â”œâ”€â”€ categories/
â”‚   â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â””â”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ views/
â”‚   â”‚   â”‚   â”œâ”€â”€ DashboardView.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ CategoriesView.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ SourcesView.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentsView.vue
â”‚   â”‚   â”‚   â””â”€â”€ SettingsView.vue
â”‚   â”‚   â”œâ”€â”€ services/               # API Clients
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”œâ”€â”€ storage/                        # Document Storage
â”‚   â””â”€â”€ documents/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh                    # Installation Script
â”‚   â”œâ”€â”€ start.sh                    # Start All Services
â”‚   â””â”€â”€ seed_data.py                # Initial Data Seeding
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ supervisord.conf            # Process Management
â”‚
â””â”€â”€ README.md
```

---

## 10. Deployment (Managed Server)

### 10.1 Systemvoraussetzungen

```
- Python 3.11+
- Node.js 20+
- PostgreSQL 15+
- Redis 7+
- Supervisor (Prozessmanagement)
- Nginx (Reverse Proxy)
```

### 10.2 Installation

```bash
#!/bin/bash
# scripts/setup.sh

# 1. Python Environment
python3.11 -m venv venv
source venv/bin/activate
pip install -r backend/requirements.txt

# 2. Frontend Build
cd frontend
npm install
npm run build
cd ..

# 3. Database Setup
createdb caelichrawler
alembic upgrade head

# 4. Environment
cp config/.env.example .env
# â†’ .env anpassen mit DB-Credentials, Azure Keys, etc.
```

### 10.3 Supervisor Konfiguration

```ini
; config/supervisord.conf

[program:caelichrawler-api]
command=/path/to/venv/bin/uvicorn app.main:app --host 127.0.0.1 --port 8000
directory=/path/to/CaeliCrawler/backend
user=www-data
autostart=true
autorestart=true
stderr_logfile=/var/log/caelichrawler/api.err.log
stdout_logfile=/var/log/caelichrawler/api.out.log

[program:caelichrawler-worker]
command=/path/to/venv/bin/celery -A workers.celery_app worker --loglevel=info --concurrency=4
directory=/path/to/CaeliCrawler/backend
user=www-data
autostart=true
autorestart=true
stderr_logfile=/var/log/caelichrawler/worker.err.log
stdout_logfile=/var/log/caelichrawler/worker.out.log

[program:caelichrawler-beat]
command=/path/to/venv/bin/celery -A workers.celery_app beat --loglevel=info
directory=/path/to/CaeliCrawler/backend
user=www-data
autostart=true
autorestart=true
stderr_logfile=/var/log/caelichrawler/beat.err.log
stdout_logfile=/var/log/caelichrawler/beat.out.log
```

### 10.4 Nginx Konfiguration

```nginx
server {
    listen 443 ssl http2;
    server_name crawler.internal.example.com;

    ssl_certificate /etc/ssl/certs/crawler.crt;
    ssl_certificate_key /etc/ssl/private/crawler.key;

    # Frontend (Static Files)
    location / {
        root /path/to/CaeliCrawler/frontend/dist;
        try_files $uri $uri/ /index.html;
    }

    # API Proxy
    location /api {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

---

## 11. Implementierungsreihenfolge

### Phase 1: Foundation (Kerninfrastruktur)

1. **Projektsetup**
   - Repository initialisieren
   - Backend-Struktur aufsetzen (FastAPI)
   - Datenbank-Schema implementieren (PostgreSQL + Alembic)
   - Redis-Anbindung fÃ¼r Celery

2. **Basis-Models & API**
   - SQLAlchemy Models
   - Pydantic Schemas
   - CRUD-Endpunkte fÃ¼r Kategorien & Datenquellen

3. **Celery Worker Setup**
   - Task Queue Konfiguration
   - Basis-Tasks definieren

### Phase 2: Crawler Core

4. **OParl-Crawler**
   - API-Client implementieren
   - Pagination & Rate Limiting
   - Dokument-Download

5. **Website-Crawler**
   - Scrapy Spider-Basis
   - Konfigurierbares Crawling
   - Playwright-Integration fÃ¼r JS-Seiten

6. **Document Processing**
   - PDF-Text-Extraktion
   - HTML-Parsing
   - Deduplizierung

### Phase 3: Intelligence

7. **Change Detection**
   - Hash-basierte Ã„nderungserkennung
   - Scheduled Checks (Celery Beat)
   - ChangeLog-Persistierung

8. **KI-Integration**
   - Azure OpenAI Anbindung
   - Extraktions-Pipeline
   - Confidence Scoring

### Phase 4: Frontend & API

9. **Admin-Dashboard**
   - Vue.js Projekt aufsetzen
   - Dashboard-Ãœbersicht
   - Kategorien-/Quellen-Management

10. **Output-API**
    - Ã–ffentliche Daten-Endpunkte
    - Export-Funktionen (CSV, JSON)
    - Webhook-System

### Phase 5: Polish & Scale

11. **Skalierung**
    - Multi-Worker Setup
    - Queue-Priorisierung
    - Performance-Optimierung

12. **Monitoring & Logging**
    - Strukturiertes Logging
    - Metriken-Dashboard
    - Alerting bei Fehlern

---

## 12. Ressourcen & Dokumentation

### Offizielle Dokumentation

- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Celery Docs](https://docs.celeryq.dev/)
- [Scrapy Docs](https://docs.scrapy.org/)
- [OParl Spezifikation](https://oparl.org/spezifikation/)
- [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/)

### Verwandte Projekte

- [Politik bei uns (GitHub)](https://github.com/politik-bei-uns) - OParl-Aggregator
- [Azure PDF Extraction Sample](https://github.com/Azure-Samples/azure-openai-gpt-4-vision-pdf-extraction-sample)

### API-Quellen

| Quelle | Typ | URL |
|--------|-----|-----|
| OParl Standard | API | https://oparl.org/ |
| GovData | Portal | https://www.govdata.de/ |
| GENESIS | API | https://www-genesis.destatis.de/ |
| XPlanung24 | Plattform | https://xplanung24.de/ |

---

## NÃ¤chste Schritte

1. **Review & Feedback** zu diesem Plan
2. **Priorisierung** der Features fÃ¼r MVP
3. **Start mit Phase 1** - Projektsetup & Basis-Infrastruktur
